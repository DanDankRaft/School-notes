- Don't need to CC Prof. on my initial email to client. Probably do that tonight.
	- im

# Regulatory Sandboxes
SEE: TRIAGA Sandbox https://capitol.texas.gov/tlodocs/89R/billtext/html/HB00149F.HTM

- Testing environment for deploying new code in a real-world environment with real data, but with less enforcement incase it violates existing regulations. These sandboxes typically have an expiration date, and require close cooperation between companies and regulators.
	- Regulatory sandbox v. pilot: a pilot is an internal launch. Doesn't require any coordination with regulatory bodies. Regulatory sandbox is an actual deployment! There's a clear regulatory hand.
- Pioneered by the UK with a crypto sandbox in 2015. But they're mostly being used for AI nowadays.

- What's the problem statement here? Not sure
- The point of regulatory sandboxes is to assemble evidence. 

- How do startups evaluate regulatory sandboxes?
	- What laws am I getting reduced liability for? What kind of remedies will I have to provide?
	- How long is the regulatory sandbox going to be?
	- What does the post-sandbox reporting entail? What do I need to report?
	- What are the eligibility requirements? Do they force me to change my product to be less adaptable to a changing market?
	- What happens if everything goes wrong? You rarely get a complete exception from existing laws, just some allowances.
		- Ex: Utah's *regulatory mitigation*. The state is working with applicants one-by-one to negotiate liability.
		- Other sandboxes typically just make a promise 
- As a regulator, how should we design regulatory sandboxes? How should we choose participants?
	- How to design participation criteria? Do we want to attract startups who might not be able to survive without the regulatory shield? Do we want to attract companies in a specific sector? Do we want to limit eligibility to companies in Texas? Think of the downstream consequences! What kind of businesses do we attract / create?



- How do we want to empower in a regulatory sandbox?
	- The state
	- Companies
	- Researchers
	- **The individual users** - how do we gather feedback from them?

- How do we ensure that regulations are actually coming out of a sandbox problem?
- The exit - what happens after? What should the point of it be?
	- Coming up with sensible consumer protections and regulations
	- 
- What kinds of issues should be subject to a regulatory sandbox? Why does AI need sandboxes, but not shotguns?
	- Quickly shifting and evolving technology - maybe it's not reasonable for the most cutting edge (where worst-case scenarios are likely and potentially harmful). But it's useful for technology that's still new but more 
	- Non-determinism: AI's behavior is not foreseeable without tons of testing and data. That makes it really tricky to evaluate in tort law!!

- To what extent do we want to prioritize power users at the expense of the typical user? or vice versa?

- How closely do we enforce the terms of the sandbox? Several options:
	- Give terms big teeth - make penalties very strong
	- Buy-in / deposit: you have to pay to join, but you get your money back if you don't violate the terms.
	- Very close oversight from AG's office - would need tons of staff!!!
	- Cure period - doesn't incentivize care bc it means there's no cost for violating terms.
	- Holding party liable if they *intentionally* violate terms of the sandbox.

- How do we make sure that the terms and oversight are sufficiently technically competent? How do we make sure the process doesn't become too political?

- It's not supposed to be a hands-off regulatory zone. It's supposed to lead to *increased* oversight!
- Potential worst-case scenarios:
	- Capture risk. If the terms of the sandbox favor some companies over others, that market change is permanent. You can't reverse that!
	- Opacity risk. To what extent can we gather the information we need to know about AI? Are companies willing to risk it?
	- What if you don't get the right participants?
	- What if you create a powerful corporate lobby with an incentive to protect themselves from public harm?

# Couch article
- CA had a law saying couches have to survive flames for 10+ seconds. Couch manufacturers would comply by putting a lot of dangerous chemicals in the couches that ended up decreasing public health.
- Policy was enacted by Bureau of Home Furnishings and Thermal Insulation - they're narrowly focused on reducing house fires and dangers thereof.
- How do we measure effectiveness of this policy intervention?
	- Test the flammability of the couch foam itself (in isolation)
	- Retroactively: they demonstrated that fire-related deaths fell after the regulation was enforced
- Takeaway: the testing mechanism you're looking to may not be the right one. Siloed approach to policy can really sour.
- Why did this issue get so long to solve?
	- The framing was difficult - we saw a reduction in couch-caused deaths! But it's hard to form a direct causal connection between chronic illnesses and the presence of chemicals in the couch.
- Takeaway: Who will stand to benefit from a policy? Especially for an *existing* policy that you're trying to repeal or amend.
- *Sacramento effect*: bc California's economy is so large, companies tend to comply with consumer product regulations in California all across the country.
	- As it applies to AI: if California passes regulations on the content of AI outputs, data will be trained to comply with California's regulations.
		- Ex: truthful readings.
- Bardock leaves out the political economy question!